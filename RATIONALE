Autograph pipelines / flow graphs:
___________________________________________

Related work: CnC-CUDA, Intel TBB

++ Resource allocation is managed automatically 
	The only reasonable choice to use the full capabilities of the underlying API
	No need to reason about temporary resources

-- Same as above
	The user may want to control precisely the resources/reuse them

++ Good optimization opportunities for temporally coherent pipelines 
	(when we do more or less the same thing on each frame)
	This is good for games, not so much for editors? maybe? 

++ can automatically extract parallelism

++ Synchronization/fencing is automatic and can be optimized

-- Becomes complex with pipelines depending on conditions
	Need higher-order nodes
-- No fine-grained user control over the scheduling of the operations
	i.e. cannot cull a branch of the flow graph manually 
-- Disruptive: forces FRP throughout the program

-- Difficult to model dependencies between successive draw operations 
	(especially when there is more than one operation)

++ Integrates well with GUI and interactive applications in general

Examples of optimizations:
+ Allocate precisely the correct number of render targets/textures (automatic ping-pong FBOs)
+ run two compute operations in parallel automatically (if there is more than one GPU/thread)
+ Automatic selection of storage memory based on the flow graph
+ Descriptor set optimization??

Proposition: a simpler set of primitives, implemented on top of autograph backends
=> no higher-order nodes (no conditionals)
=> individual pipelines, can temporarily deallocate their resources
=> more like a memory management and synchronization technique
	(maps very well to Vulkan's render passes)



GLSL header gen
___________________________________________

Parse GLSL, generate C++ header file with type-safe bindings
One entry for each shader defined in the FX file

FX file: should it contain Blend/Depth/Rasterizer states?
VS #include ?



GPU promises and futures
___________________________________________

The rendering pipeline is dynamic. The individual rendering tasks are 
re-created on each frame.

-+ Resources must be managed manually
	Most notably: the residency of memory resources

-- Synchronization must be managed manually
	Although this is less painful with asynchronous primitives (promises and futures)

-- Hard to optimize for the backend API
	The backend does not have global knowledge of the rendering pipeline: 
	it cannot take advantage of the fact that the sequence of operations do not change
	between frames.
	As a result, must resort to caching and heuristics.
	=> how to take advantage of bindless/descriptor sets?

++ Integrates well with existing libraries
	No need for a complex machinery

-? Harder to make changes to an existing pipeline
	Not sure about this one: the only thing harder is that the user has to allocate 
	the new resources

++ parallel execution is explicitly controlled by the user

+? Barring complex pipeline optimizations that could not be done manually, probably more efficient as a whole

Ideas:

* Model GPU operations using asynchronous primitives (GPU-futures)
	
	// add command to command list for target
	draw(GPUAsync<Target>, state, bindings) -> GPUAsync<Target>

	GPUAsync A;
	GPUAsync B;

	// The order of these two operations is undefined!
	// They can even be done in parallel
	B = draw(A, something);
	draw(A, something else);

	// the correct way is:
	B = draw(A, something)
	C = draw(B, something)
	// which is rather verbose

	// but then you should be able to do things like:
	B = draw(A, something)
	C = draw(B, something)
	D = draw(C, B, ...)
	// and the system will automatically allocate a temporary render target
	// and synchronize on B
	// OR:
	// draw(X, ...) consumes its argument (takes a rvalue reference)
	// thus, the temporary copy is explicit
	// (this would work very well with Rust...)
	
	// extract the value of a GPU async with:
	GPUAsync<T> A;
	A.sync() -> T // forces CPU/GPU sync 
	// or you could chain a CPU asynchonous operation

	// IMO, this is more readable than the flow-graph approach
	// Especially concerning draw calls 
	// (need the concept of 'draw lists' with the flow-graph, to model 
	//  dependencies between a varying number of successive draw calls)
	// The resources can be GPUAsyncs too! (results of GPU operations)

* Multiple render targets?
	GPUAsync<Frame> has special members that can extract 
	a GPUAsync<Texture> from a render target

* Type-safe resource binding with variadic templates

* Type-safe shaders


Texture2D
Texture3D

Texture2DView<T, D> -> bindable as image or render target, but not as a texture
Texture2DView<const T, D> -> bindable as texture only

GPUAsync<Texture2D>().view() -> GPUAsync<Texture2DView>

Sampler -> ...



GPUAsync<T,D> 		: generic buffer of type T, living between the GPU and the CPU
//GPUAsync<TexturenD<T,D>,D> : specialization for texture types
GPUAsync<TexturenDView<T,D>,D> : specialization for texture view types
GPUAsync<Surface<D, T...>> : specialization for surfaces


GPUAsync<T&> or GPUAsync<array_ref<T>>

Problem with GPUAsync<array_ref<T>> : what if the size of the array is only known once the async operation is complete?

GPUAsync<array_ref<T>> specialization
- underlying buffer
- offset
- size in Ts


// should be:

GPUAsync<Surface<...>> v1 = context.getScreen();

v1 = draw(move(v1), ...)

	* move(v1)
	* draw(v1&&)
		* for each parameter
			* backend.bind(v1.detail, GPUAsync)
				* if the async object has an outstanding write command list, flush it
				* if the async object has pending read commands (is referenced by a command list somewhere)
				  and if we are writing to this object
				  	* fence on completion of read commands
				* add bind commands to detail.commandList (optimize redundancies)
		* add draw command to detail.commandList
		* copy v1 to result
		* return result 

Note: two GPUAsyncs can share the same command list
e.g. a texture_nd_view and a surface

-> question: when to drop the command list?
and when to allocate it?

v1 = draw(move(v1), ...)
...
v1 = draw(move(v1), ...)	// update v1


ShaderResource<T>	-> binds as an uniform buffer
ShaderResource<TexturenD<T>> -> gets a constant texture view, bind as texture with default linear sampler
ShaderResource<TextureUnit<TexturenD> > -> bind as texture unit + sampler pair
ShaderResource<TexturenDView > -> bind as image unit
ShaderResource<....> -> ....
ShaderResource<SSBO<...>> -> bind as SSBO (openGL specific)


XXX create multiple command lists in parallel?



// Drawables:
DrawIndexedMesh(Buffer<T>, Buffer<Index>)
DrawIndexedMesh(...)

Mesh(primitive type, buffers...)
IndexedMesh(primitive type, index buffer, buffers...)
InstancedMesh(primitive type, num instances, first instance, buffers...)
InstancedIndexedMesh(primitive type, num instances, first instance, index buffer, buffers...)

Buffer can be a buffer object or a view (GPUAsync<array_ref<T>>)

When to flush the command buffer?
when an async<surface> or async<texture> is joined?
Or immediately?

RAII and destructors:
- Destructor of the generic object is called
	backend.destroyXXX(object) is called
	detail objects are default-constructible and must not have a custom destructor?
	=> must be POD types

textures, surfaces, etc. are move-only types
surfaces do not own textures
but for convenience, there should be a way to create a surface without
first creating the underlying textures.
=> Use a ResourceScope as the owner of the data
	created object is bound to the scope

backend.createResourceScope(...)
backend.createFencedResourceScope(...)


Parallel operations:

1. v1 = compute(...)
2. v2 = compute(v1)
3. v3 = compute(v1)


2. v1 last written by CQ#1, no pending reads -> schedule on CQ#1
3. v1 last written by CQ#1, one pending read on CQ#1 -> schedule on CQ#2





About destructors and resource lifetimes
_____________________________

A common mistake:

	void thing(Device& d) 
	{
		auto param = Uniform(d, ...)
		draw(..., param, ...)
		// param drops here, but the draw operation may not have finished!
		// must extend the lifetime of param -> shared_ptr's ?
		// OR (gfx-rs): garbage-collect unused resources at end of frame (still using shared-ptrs)
		// OR: require the user to explicitly specify the lifetime of the source (using device.frame_scope.create<Uniform>())
	}

The lifetime of GPU resources are almost never tied to a function scope.
They live at least as long as the draw call, and maybe more if a reference to them is kept 
between frames.

Examples of scopes: 
	- lexical scope lifetime (almost never used)
	- object lifetime
	- frame lifetime
	- lifetime until fence
	- static lifetime

Once the scope object is destroyed, all objects bound to the scope become invalid
(but can still be destroyed)



About shaders
_____________________________

Nomenclature:
	Vertex shader (VS)
	Geometry shader (GS)
	Pixel shader (PS) -> Fragment shader
	Hull Shader (HS) -> Tess control
	Domain Shader (DS) -> Tess eval
	Compute Shader (CS)

do not expose individual objects

Shader resource types:

Textures (GL/D3D11)
Constant buffers (GL:uniforms/D3D11:cbuffers)
	UniformXXX binders
Read/write buffers (GL:SSBO/D3D11:RWBuffers)
	RWBuffer<T>, RWBufferSlot<T>, 
Read/write textures (GL:imagenD/D3D11:RWTexturenD)
	RWTexturenD<T>, RWTexturenDSlot<T>



About bindings
_____________________________

Divided in 3 parts:

	- Pipeline state object -> either PipelineState or ???
	- resources (textures, buffers, vertex buffers)
	- draw call parameters


Pipeline state:
	-> just pass a pipeline state object

Draw call:
	- MeshPart
	- DrawArrays
	- DrawIndexed
	- DrawIndexedInstanced
	- DrawIndirect

	TextureUnit(0, TextureType)
	BufferSlot(...)


First simplification pass
_____________________________

TOP PRIORITY ITEMS
* Do not use raw arrays in predefined pixel types
	use std::array
* Render target specification and caching (render target descriptors, etc.)
	RenderTargets(depth, color0, color1, ...) -> RenderTarget<Depth, Colors...>
	MakeRenderTargetNoDepth(color0, color1, ...) -> ...

	Binding: RenderTarget(tex), DepthStencil(...) ... VS one RenderTargets(depth, color, ...)
	// there must be at least one target, so only one bind site:
	RenderTargets(depth_view, color_views...)
	
	backend: setRenderTargets(RenderTargetView: union, DepthStencilView: union)

	// color0, color1 are texture subresource views (textures, texture views of a layer, etc.)
	backend.createRenderTargetDescriptor(span<RenderLayer> layers)
	RenderLayer: union (texture2D mip level, texture2D view, ...)

* Command list semantics (stateless?)
	Redundant state elimination should be done in API wrapper
* Reform of input observables
* asynchronous command list generation (see brush task: generate draw commands in a coroutine, yield command buffers)


* Blocking version of UploadBuffer::upload 
* kUniformBufferOffsetAlignment should be dynamically queried
* Upload buffers should return correctly aligned regions depending on the use
	Should not need to specify the alignment manaully
	-> explicitly typed buffers / tagged buffers ?
	```
	uploadBuf.allocateConstantBuffer<T>(value...) -> BufferSlice<T>
	```
* Normalize the order of template parameters (D, T...)
* Const-correctness in buffer and buffer slice types
	(buffer<const T> is immutable, buffer<T> is mutable and binds as a RWBuffer, as_const method that returns a const buffer slice)
* Extract boilerplate for samples (pipeline creation, etc.)
	- common shaders
	- resource loading (meshes, textures)
	- common samplers
	- vertex types
	- rendering of meshes
* Force debug breakpoint
* Break on shader compilation error
* CRTP for the backend
* Bikeshedding: rename Backend to Driver?
* Bikeshedding: snake_case names?
* Simplify the passing of handles to backend functions
* pushDataToUploadBuffer is very poorly named
* move input module out of extras
* Binders: viewports
* Binders: scissor rectangle
* replace associated consts with static constexpr **functions**
*?? Simpler pipeline creation (for OpenGL)
* move blur in imageproc extra
* Texture types should be parameterized by a typeless surface format (number of channels, bits per channel)
	* Concrete types in texture views


DONE Rename source files in backend
DONE Clean the samples boilerplate
DONE Use an unique cmake file for the samples
DONE Bikeshedding: lowercase file names
DONE Samples: automatically find the root asset folder
DONE Remove OpenGLPixelType.hpp
DONE Remove templated methods in backend (createTexturenD<TPixel>)
DONE Clean indentation (LLVM-style, 2 spaces)
DONE Remove refcounted resources




Formalization: usage of glm vector types
_____________________________

Minimum dependencies are always good. Consider removing glm from the requirements?
(so that people can use eigen instead)
Issue: cannot include predefined pixel types in the library. (except array types)
	=> that's good
People should define their own pixel types


Formalization: binders
_____________________________

Requirements in terms of concepts
	concept ShaderResource
	concept RenderTarget
	concept Drawable



Feature: type-erased texture types
_____________________________

Specialization of texturenD types
To be used when the pixel type is not known at compile time



Feature: multiDraw 
_____________________________


Feature: multiDrawIndirect
_____________________________


Feature: dispatchIndirect
_____________________________



Feature: clear/copy command
_____________________________

ag::clear
Clear textures, surfaces, buffers, etc.
Question: clear value should be raw bits or float[4]?
Facts about texture data:
	The layout of texture data is generally unknown. (tiling?)
	The driver or GPU must sometimes perform a conversion on upload.
	In general, there is no direct mapping between a pixel's color and a sequence of bits in texture data 
		(see compressed formats)
	Two types of conversion: 
		layout conversion (tiling) => opaque, GPU-specific
		format conversion (different pixel types) => avoid
	Possible format conversions vary between APIs, drivers and GPU

	For one texture format, 2 different associated types:
		- sample_type: interpolated/sampled pixel type
		- storage_type: type of elements in a texture 
			can be different from storage_type for compressed formats

        Additionnally, one format can be interpreted differently:
                - surface_format: number of channels and bits per channel (RGBA8, RGB8, RGB10A2)
                - channel_format: interpretation of channel data (float, int, normalized int)
        => textures can be fully-typed (as it is the case now) or typeless (special typeless formats)
            => specify full format at bind time?
        => use case: write to a texture as integer, bind and sample as unorm

Texture transfers:
glTexSubImage may copy the provided texture data to gl-managed memory and keep
it until the texture is ready to be copied over.
My guess is that D3D12, Vulkan and friends do no such thing (no extra memory allocations),
and we have to manage this "staging buffer" ourselves.

Layout of texture upload data:
Must follow alignment constraints (each scanline must be correctly aligned)
cf. PIXEL_UNPACK_ALIGNMENT

=> Need a special span type for correctly aligned image data
ImageData<T>, which is a view of an ImageBuffer<T>, with special methods
to write a pixel / a scanline / a block.



Rationale: Texture types (shared by APIs)
_____________________________

Texture1D, 2D, 3D
TextureCubeMap
Texture1DArray
Texture2DArray
TextureCubeMapArray

subresources: mipmaps + array layers

group all types into a shared handle type?
TextureHandle
BufferHandle
cannot enforce polymorphism/inheritance to the handle types

Arguments for having different handle types for texture types:
- type safety? 
	but the backend API is not exposed
- consistency

Arguments against:
- Mismatch with APIs (OpenGL, Direct3D 12)
- Need union types for some APIs (ex: copyTextureRegion(union src, union dest))
- More API variants for resource binding 

=> It doesn't really matter to the library user
=> choose what's more convenient to code with (only one texture handle type)


Feature: Command lists
_____________________________

* For multithreaded command submission
* Semantics of bindings?


Feature: Upload buffers
_____________________________

Currently hidden
* Expose upload buffers to the user?


Binders
_____________________________


// Shortcut draw binders (bind vertex buffers and emit the drawcall)
DrawArrays(PrimitiveType, vertex_source)
DrawIndexed(PrimitiveType, vertex_source, index_source)

// Base draw binders
DrawArrays(PrimitiveType, first, count)
DrawIndexed(PrimitiveType, first, count, base_vertex)

// Vertex buffers
VertexBuffer(vertex_buffer)
VertexBuffer(untyped_buffer, offset, stride)
IndexBuffer(buffer)
IndexBuffer(untyped_buffer, offset, type)




Feature: Compute pipelines and compute API
_____________________________

ag::compute(device, pipeline, <work group config>)



Feature: Render targets
_____________________________

Like binders, except for render targets:
Surface(tex0, tex1, ..., depth tex, stencil tex / depth-stencil tex)
Surface(TextureLayer(xxxx))
SurfaceCube(...)
TextureRT()
TextureLayerRT()
TextureCubeFaceRT()

draw(tex0, ...)
draw(Surface(tex_color, tex_depth), ...)
draw(screen, ...)
draw(tex_cube, ...)
draw(Surface(tex_color, tex_depth, tex_stencil))


Feature: D3D12 backend
_____________________________

medium-priority


Feature: Partially resident textures
_____________________________

Cool


Feature: Texture blit operations
_____________________________

Copy buffer data (with the same underlying type)
Copy texture data to another (with the same pixel type)
Copy texture data to RenderTarget? No: this is a rendering operation

ag::copy(commandlist, buffer_src, buffer_dst)
ag::copy(commandlist, texture_view_src, texture_view_dst)

Mappable buffer vs unmappable buffers
Mapped slices (accessible by the CPU) and buffer slices (not accessible by the GPU)
=> NO: hide mapped slices (they should only be WRITTEN by the CPU, not read)
=> access only through upload buffers

RawBuffer
Buffer<T>
Buffer<T[]>
RawBufferSlice
BufferSlice<T>
BufferSlice<T[]>
- MappedRawBufferSlice
- MappedBufferSlice<T>
- MappedBufferSlice<T[]>


Feature: Data readback
_____________________________

Needed! High priority (can be only synchronous for now)
Asynchronous VS synchronous
something like 
	texture.readBack([](auto view) {...})	// executed when the data is available. no syncing
or, for consistency, a free function:
	ag::readBack(commandlist, texture, [](auto view) {...})



Feature: Separate window creation from backend
_____________________________

Move these in separate libraries


Feature: graphics debugging and profiling
_____________________________

Record/replay frame



Feature: more texture types
____________________________

* Mip maps
* Cube maps
* texture arrays


Feature: GPU flow graphs
_____________________________

Intel TBB for the GPU
may require backend support 
(implement as an optional feature)


Feature: autograph-shaders
_____________________________

Shaders in C++ code (as lambdas), extracted by clang and transpiled to GLSL
Issue: host compiler, generation of correctly aligned data structures
At first: separate C++ code files (modularized)


Feature: extras
_____________________________

in namespace ag::extras, separate library

math: lightweight math support library
	ONLY: perspective transformations 

interactive: framework for interactive applications
	observables, reactive operations => use RxCpp
	application events: application close, etc.
	forward events (backend-specific) to input handlers, rendering contexts and windows
	not an extra!
        must provide escape hatches
        must be accessible by the application if needed
        => application creates the object, pass reference to ag subsystems
        => in charge of creating the main window/main device
		=> call API to get/set a render output surface

	ag::api::glfw::GLFW_api
	ag::api::x11::

	application frameworks: 
		- GLFW
			using GraphicsDriver = OpenGLDriver
		- Native X11
			using GraphicsDriver = OpenGLDriver
		- Native Win32
			using GraphicsDriver = Win32GLDriver / DirectX11Driver / DirectX12Driver
                - WinRT
        naming: core? interactive? framework?

input: library to handle various input devices
	mouses, keyboards, gamepads and graphics tablets
	input handlers receive events (opaque) from the application framework
	and translate them into input events
      (note: merge with ag::interactive)

camera: camera control from input events

mesh: generic mesh library
	use assimp for loading
	type-safe mesh types
	automatic normal generation
	smoothing groups
image_io: image loader, designed to be used easily with ag::Texturexx types
	depends on: stb_image? gli?
	image manipulation library? (video++ comes to mind)
text: draw text on a render target
	depends on freetype?
sdftext: signed distance field font rendering
	dependencies?
sprite: texture/sprite blitting operations
	no dependencies
postfx: post-process effects
	no dependencies
video: render video?
vector: vector drawing library (2D)
	nvg? 
	vector::draw(): extensions to ag::draw
visualization: visualizers/debuggers for various structures
	debug textures, structured buffers, meshes, etc.
	ex: draw mesh in wireframe
		draw mesh with its normals, tangents, etc.
gui: user interface
graph: graphs (histograms, plots, etc.)
scene: scene graph rendering?
shade_graph: load shader graphs and flow graphs from a description file
assets: asset management (load assets by key, cache resources)
ecs: entity-component system
pcg: procedural content generation utilities?
animation: animation curves, CPU/GPU skinning and skeletal animation
	use assimp to load motion files
terrain: heightmap rendering
canvas: digital painting canvas structures and operations
	- layers (dirty region management)
	- blend modes
	- brush engine
	- support for undo buffer
	- support for painting in texture maps
	(high priority)


Needed for canvas
_____________________________

Texture upload/download
Painter:

- Brush engine
	-> draw to stroke mask (pipeline)
	-> compose stroke (compute pipeline)

Brush(draw_pipeline, compose_pipeline, parameters)

Zoom
struct CanvasView:
	- getTransform
	- zoomLevel
	- position on screen
	- ref to canvas?
	- transform input position



Feature: primitive types encoded in the vertex buffer type 
_____________________________

and in general, more things encoded in the type
like, if the mesh should be used with index data, or as a vertex array, etc.
=> mesh type?
Low priority


Feature: event and input 
_____________________________

Based on observable objects
Observable object:
	Observable<T>
	can subscribe to changes
Events: fed to observables

One-way:
	signal -> receiver
Two-way:
	data binding

auto key_press_event = keyboard.onKey(Key::F1).map(Key::Pressed)

observables:
	raw mouse/pointer events (delta values)
	keyboard events
	mouse button events

published behaviors: 
	mouse button state
	mouse cursor position
	

Rationale: on variable scopes 
_____________________________

Some variables make sense only when drawing a stroke
Ideally, they should have coroutine scope.
Instead: create object that holds all this temporary state.
A paint tool is a coroutine that waits on input and draw events



Rationale: on gsl::span
_____________________________

Complex and impractical => replace?



Vulkan notes
_____________________________

Descriptors? 
In vulkan: the process is:
	(pre-render) generate descriptor set layouts
	(render) create descriptor sets, bind them
	=> best practices: 
		create one descriptor set, update between each draw call?
			=> matches the current draw API, where bindings are dynamic
		VS
		create descriptor sets upfront
			=> matches the RenderPass API
		VS
		cache descriptor sets
			=> complex implementation

Same choices with image/buffer views:
	create them on-the-fly
	VS
	create them once up front
	VS
	cache them



Render pass API
_____________________________

Suddenly makes sense with vulkan
Simplify:
* no conditionals (a render pass is a straight branch)
* inputs are dynamic
* draw operations: either pre-recorded command lists or dynamically generated draw call streams

Value nodes:
* Constant (uniform) buffers
* RWBuffers
* textures (sampled images)
* RWTextures (images)
=> constant or dynamic

Operation nodes:
* clear
* draw
* copy
* upload
* readback

New value nodes:
Image<Type>
Buffer<Type>

Renderpass dependency graph is a DAG, all branches are scheduled and evaluated.
Scheduling and execution done by the backend


auto renderpass = draw(clear(rendertarget(0)), pipeline, drawcalls(0), texture1D, texture(0))

// => returns a function object?
renderpass: RenderPass<RenderTargetType, ... DrawCallStream, Texture2D<PixelType>> 

clear<RT>(constant vec4) -> clear<RT, constant>
clear<RT>(placeholder<vec4>) -> clear<RT, vec4>

draw<N, Resources...>(RT<params>, ) -> auto  

=> a variable number of draw resources
=> each draw resource can have a variable number of dynamic parameters
=> Arcane template wizardry
	=> does not scale

renderpass(rendertarget, mesh, texture) 

command lists => multithreading
applying renderpasses is cheap, creating command buffers is not
cannot use stateless ag::draw with renderpasses (ag::draw re-specifies all the state each time)
	use ag::draw with StatePack? 
	StatePacks: bundled set of bindings that never change => re-use descriptors
		=> corresponds to Vulkan's DescriptorSets
		=> also very good for state-sorting
		=> type-erased
		=> with renderpasses: partial application of ag::draw
			=>  
    Bikeshedding: StatePack, BindPack, StateSet, BindSet, BindingSet, StateBundle

a renderpass is a callable; arguments are dynamic state
static (creation-time state) is set on construction 

Construction of a renderpass:
- execute the provided code with dummy arguments, extract graph




Shaders in C++
_____________________________

Goal: same as C++ AMP, but targets Vulkan and SPIR-V

* Separate code that must be run on GPU (annotation?)
* Where to do it: 
	- C++ level?
	- LLVM bitcode level?
	=> propagate C++ attributes to LLVM bitcode
	=> copy C++ AMP for primitives?
* Shaders as functions (possibly templates)

* Issue: parameters/bindings shared between VS/GS/PS
	-> meta-shaders?
	-> pipeline descriptions

[[pipeline]]
auto renderMesh(meshIn, meshOut, sigma)
{
	auto vstream = VS(...)
	auto rasterizer_out = Rasterizer(state, ...)
	PS(rasterizer_out, ...)
}



About synchronization:
_____________________________

OpenGL: commands are executed sequentially (no out-of-order execution)
Vulkan: commands within a command list, queue submissions can happen out-of-order

Autograph: 2 choices
	- maintain implicit ordering of operations in a command list
	- modify API to expose explicit synchronization
	- remove the command list API, use only renderpasses (back to very first design...)

ag::draw returns a GPUFuture (back to first design...)


Composable renderpasses:
_____________________________

A renderpass is a function generator with three levels of parameters:

(- template parameters: compilation-time)
- renderpass parameters: renderpass creation time (runtime)
- dynamic parameters: draw call time (runtime)

// callable object
RenderPass<ReturnType, DynamicState...>

A render pass can output multiple images



auto initRenderPass() 
{
	
	auto rp = makeRenderPass([&](ag::image<>, ag::image<>, ...) {

	});

	makeRenderPass(...) -> execute lambda with dummy parameters for construction

	OR:

	RenderPass is a class, with operator()
}


ag::renderpass::RenderPassBase (type-erased)
	- required resources (images, samplers, descriptors)



ag::renderpass::RenderPass<Output, Inputs...>

ag::renderpass::ClearNode
	- input node: Image 
	- output node: Cleared image


ag::renderpass::RawImage
	- dimension, width, height, depth, 
	  format, mips, array count, faces
ag::renderpass::Image1DNode<Pixel>
ag::renderpass::Image2DNode<Pixel>
ag::renderpass::Image3DNode<Pixel>
	- width, height, depth, format
ag::renderpass::Image1DArrayNode<Pixel>
ag::renderpass::Image2DArrayNode<Pixel>

ag::renderpass::DrawNodeBase
ag::renderpass::DrawNode<Generator>
	- source image (resource entry in renderpass)
	- state pack
	- draw call generator (callable)

ag::renderpass::CopyNode
	- source image (entry)


Example with the painter pipeline:


auto clearedCanvas = rp::clear(width, height, color);
auto baseColorPreview = preview(clearedCanvas, ...)

To the backend: pass a directed graph of resources & nodes

Pb: GPU nodes that read and write the same texture simultaneously?
=> in-place modification
=> doable


Extra: pipeline cache
_____________________________


Concurrent projects:
* autograph
* shaders in c++ for autograph
* autograph: input
* autograph: renderpasses
* painter
* painter: coroutines
* painter: reactive UI

=> proposition:
	evaluer C#
	autograph en C#/.Net


conserver autograph, specialiser pour opengl 4.5
(abandonner backends multiples)


C# vs C++ pour du code générique
_____________________________

* templates => generics